{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultilayerPerceptron.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1xrZjaRkF94a4Qi-Zsf11VSB9gFIzlpdp","authorship_tag":"ABX9TyOjNnlyJ1ta+C63JJTlbCrU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"dbdMWBrXxeBm"},"source":["from IPython.display import clear_output\n","%cd /content/drive/MyDrive/collab_sandbox/learn_DL/dive_into_dl/\n","# !pip install colabcode\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vT19w8ieym6E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638924900752,"user_tz":-180,"elapsed":6428,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"29f1d7d8-fafa-44f1-e928-614e45633a09"},"source":["import pandas as pd\n","import torch\n","import numpy as np\n","from torch.utils import data\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import matplotlib.pyplot as plt\n","import seaborn.apionly as sns\n","from sklearn.model_selection import KFold\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n","  warnings.warn(msg, UserWarning)\n"]}]},{"cell_type":"code","metadata":{"id":"svVf77HIyvFO","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1638924902315,"user_tz":-180,"elapsed":506,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"3d4f0ef6-d157-4829-ed7b-7c632674eaf2"},"source":["dataset = pd.read_csv(\"./data/HousePricesKaggle/train.csv\")\n","\n","drop_columns = 'Id'.split()\n","dataset = dataset.drop(columns=drop_columns)\n","\n","select_columns = dataset.dtypes[dataset.dtypes != 'object'].index.tolist()\n","select_columns = list(set(select_columns) - set(['SalePrice']))\n","\n","dataset[select_columns] = dataset[select_columns].apply(lambda x: (x - x.mean()) / (x.std()))\n","\n","dataset[select_columns] = dataset[select_columns].fillna(dataset[select_columns].mean() / dataset[select_columns].std())\n","\n","# encode to one hot https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe\n","OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","\n","object_columns = dataset.dtypes[dataset.dtypes == 'object'].index.tolist()\n","OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(dataset[object_columns]))\n","\n","OH_cols_train.index = dataset.index\n","num_X_train = dataset.drop(object_columns, axis=1)\n","\n","dataset = pd.concat([num_X_train, OH_cols_train], axis=1)\n","\n","dataset.head()\n"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MSSubClass</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>OverallQual</th>\n","      <th>OverallCond</th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>MasVnrArea</th>\n","      <th>BsmtFinSF1</th>\n","      <th>BsmtFinSF2</th>\n","      <th>BsmtUnfSF</th>\n","      <th>TotalBsmtSF</th>\n","      <th>1stFlrSF</th>\n","      <th>2ndFlrSF</th>\n","      <th>LowQualFinSF</th>\n","      <th>GrLivArea</th>\n","      <th>BsmtFullBath</th>\n","      <th>BsmtHalfBath</th>\n","      <th>FullBath</th>\n","      <th>HalfBath</th>\n","      <th>BedroomAbvGr</th>\n","      <th>KitchenAbvGr</th>\n","      <th>TotRmsAbvGrd</th>\n","      <th>Fireplaces</th>\n","      <th>GarageYrBlt</th>\n","      <th>GarageCars</th>\n","      <th>GarageArea</th>\n","      <th>WoodDeckSF</th>\n","      <th>OpenPorchSF</th>\n","      <th>EnclosedPorch</th>\n","      <th>3SsnPorch</th>\n","      <th>ScreenPorch</th>\n","      <th>PoolArea</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SalePrice</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>...</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","      <th>233</th>\n","      <th>234</th>\n","      <th>235</th>\n","      <th>236</th>\n","      <th>237</th>\n","      <th>238</th>\n","      <th>239</th>\n","      <th>240</th>\n","      <th>241</th>\n","      <th>242</th>\n","      <th>243</th>\n","      <th>244</th>\n","      <th>245</th>\n","      <th>246</th>\n","      <th>247</th>\n","      <th>248</th>\n","      <th>249</th>\n","      <th>250</th>\n","      <th>251</th>\n","      <th>252</th>\n","      <th>253</th>\n","      <th>254</th>\n","      <th>255</th>\n","      <th>256</th>\n","      <th>257</th>\n","      <th>258</th>\n","      <th>259</th>\n","      <th>260</th>\n","      <th>261</th>\n","      <th>262</th>\n","      <th>263</th>\n","      <th>264</th>\n","      <th>265</th>\n","      <th>266</th>\n","      <th>267</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.073350</td>\n","      <td>-0.207948</td>\n","      <td>-0.207071</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>1.050634</td>\n","      <td>0.878367</td>\n","      <td>0.509840</td>\n","      <td>0.575228</td>\n","      <td>-0.288554</td>\n","      <td>-0.944267</td>\n","      <td>-0.459145</td>\n","      <td>-0.793162</td>\n","      <td>1.161454</td>\n","      <td>-0.120201</td>\n","      <td>0.370207</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>0.911897</td>\n","      <td>-0.950901</td>\n","      <td>0.992066</td>\n","      <td>0.311618</td>\n","      <td>0.350880</td>\n","      <td>-0.751918</td>\n","      <td>0.216429</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-1.598563</td>\n","      <td>0.138730</td>\n","      <td>208500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.872264</td>\n","      <td>0.409724</td>\n","      <td>-0.091855</td>\n","      <td>-0.071812</td>\n","      <td>2.178881</td>\n","      <td>0.156680</td>\n","      <td>-0.429430</td>\n","      <td>-0.572637</td>\n","      <td>1.171591</td>\n","      <td>-0.288554</td>\n","      <td>-0.641008</td>\n","      <td>0.466305</td>\n","      <td>0.257052</td>\n","      <td>-0.794891</td>\n","      <td>-0.120201</td>\n","      <td>-0.482347</td>\n","      <td>-0.819684</td>\n","      <td>3.947457</td>\n","      <td>0.789470</td>\n","      <td>-0.761360</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>-0.318574</td>\n","      <td>0.600289</td>\n","      <td>-0.101506</td>\n","      <td>0.311618</td>\n","      <td>-0.060710</td>\n","      <td>1.625638</td>\n","      <td>-0.704242</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-0.488943</td>\n","      <td>-0.614228</td>\n","      <td>181500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.073350</td>\n","      <td>-0.084413</td>\n","      <td>0.073455</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>0.984415</td>\n","      <td>0.829930</td>\n","      <td>0.322063</td>\n","      <td>0.092875</td>\n","      <td>-0.288554</td>\n","      <td>-0.301540</td>\n","      <td>-0.313261</td>\n","      <td>-0.627611</td>\n","      <td>1.188943</td>\n","      <td>-0.120201</td>\n","      <td>0.514836</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>-0.318574</td>\n","      <td>0.600289</td>\n","      <td>0.911061</td>\n","      <td>0.311618</td>\n","      <td>0.631510</td>\n","      <td>-0.751918</td>\n","      <td>-0.070337</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>0.990552</td>\n","      <td>0.138730</td>\n","      <td>223500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.309753</td>\n","      <td>-0.413838</td>\n","      <td>-0.096864</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>-1.862993</td>\n","      <td>-0.720051</td>\n","      <td>-0.572637</td>\n","      <td>-0.499103</td>\n","      <td>-0.288554</td>\n","      <td>-0.061648</td>\n","      <td>-0.687089</td>\n","      <td>-0.521555</td>\n","      <td>0.936955</td>\n","      <td>-0.120201</td>\n","      <td>0.383528</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>-1.025689</td>\n","      <td>-0.761360</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>0.296662</td>\n","      <td>0.600289</td>\n","      <td>0.789553</td>\n","      <td>1.649742</td>\n","      <td>0.790533</td>\n","      <td>-0.751918</td>\n","      <td>-0.175988</td>\n","      <td>4.091122</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-1.598563</td>\n","      <td>-1.367186</td>\n","      <td>140000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.073350</td>\n","      <td>0.574436</td>\n","      <td>0.375020</td>\n","      <td>1.374324</td>\n","      <td>-0.517023</td>\n","      <td>0.951306</td>\n","      <td>0.733056</td>\n","      <td>1.360357</td>\n","      <td>0.463410</td>\n","      <td>-0.288554</td>\n","      <td>-0.174805</td>\n","      <td>0.199611</td>\n","      <td>-0.045596</td>\n","      <td>1.617323</td>\n","      <td>-0.120201</td>\n","      <td>1.298881</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>1.389547</td>\n","      <td>-0.211381</td>\n","      <td>1.527133</td>\n","      <td>0.600289</td>\n","      <td>0.870558</td>\n","      <td>1.649742</td>\n","      <td>1.697903</td>\n","      <td>0.779930</td>\n","      <td>0.563567</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>2.100173</td>\n","      <td>0.138730</td>\n","      <td>250000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 305 columns</p>\n","</div>"],"text/plain":["   MSSubClass  LotFrontage   LotArea  OverallQual  ...  264  265  266  267\n","0    0.073350    -0.207948 -0.207071     0.651256  ...  0.0  0.0  1.0  0.0\n","1   -0.872264     0.409724 -0.091855    -0.071812  ...  0.0  0.0  1.0  0.0\n","2    0.073350    -0.084413  0.073455     0.651256  ...  0.0  0.0  1.0  0.0\n","3    0.309753    -0.413838 -0.096864     0.651256  ...  0.0  0.0  0.0  0.0\n","4    0.073350     0.574436  0.375020     1.374324  ...  0.0  0.0  1.0  0.0\n","\n","[5 rows x 305 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"z3629TUOq-_N"},"source":["# fig, ax = plt.subplots(figsize=(40,40))\n","# corr = dataset_train.corr()# plot the heatmap\n","# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True), ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3j48Kai1umU","executionInfo":{"status":"ok","timestamp":1638886619262,"user_tz":-180,"elapsed":318,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["# def encode_categorical(dataset, columns, full_dataset):\n","#   dataset = dataset.copy(deep=True)\n","#   # print(dataset)\n","#   dataset_cols = dataset.columns.values\n","#   for column in columns:\n","#     if column in dataset_cols:\n","#       values_list = full_dataset[column].values.tolist()\n","#       list_columns = sorted(list(map(str, list(set(values_list)))))\n","#       list_columns_renamed = [f'{column}_{item}' for item in list_columns]\n","#       dict_columns = {item: i for (i, item) in enumerate(list_columns)}\n","      \n","#       # print(dataset[column].)\n","#       dataset_values = dataset[column].tolist()\n","#       for i, value in enumerate(dataset_values):\n","#         value = str(value)\n","\n","#         ohe_hot = [0 for _ in range(len(list_columns))]\n","#         if dict_columns.get(value):\n","#           ohe_hot[dict_columns.get(value)] = 1\n","\n","#         # slow operation!\n","#         dataset.loc[i, list_columns_renamed] = ohe_hot\n","#       dataset = dataset.drop(columns=[column],) \n","  \n","#   return dataset"],"execution_count":110,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFlJlWaYGwZJ"},"source":["# dataset_train.dtypes[dataset_train.dtypes == 'object'].index.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAbktca95BeZ"},"source":["# categorical_labels = dataset_train.dtypes[dataset_train.dtypes == 'object'].index.tolist() #list(set('MSSubClass PavedDrive MSZoning Street\tAlley\tLotShape\tLandContour\tUtilities\tLotConfig\tLandSlope\tNeighborhood\tCondition1\tCondition2\tBldgType\tHouseStyle RoofStyle\tRoofMatl\tExterior1st\tExterior2nd\tMasVnrType ExterQual\tExterCond\tFoundation\tBsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1 BsmtFinType2 Heating\tHeatingQC\tCentralAir\tElectrical KitchenQual Functional FireplaceQu\tGarageType GarageFinish PoolQC\tFence\tMiscFeature SaleType\tSaleCondition GarageCond GarageQual GarageType GarageFinish\tGarageQual GarageCond'.split()))\n","# dataset_train = encode_categorical(dataset_train, categorical_labels, dataset)\n","# dataset_test = encode_categorical(dataset_test, categorical_labels, dataset)\n","# dataset_train.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbAsU_jq8f_4","executionInfo":{"status":"ok","timestamp":1638910777796,"user_tz":-180,"elapsed":295,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["# len(dataset_train.columns) == len(dataset_test.columns), len(dataset_test.columns), len(dataset_train.columns)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRTuFNJGJ975","executionInfo":{"status":"ok","timestamp":1638910782073,"user_tz":-180,"elapsed":301,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["# dataset_train.head(5)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdwDbNtw9Gqb","executionInfo":{"status":"ok","timestamp":1638910785291,"user_tz":-180,"elapsed":299,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["# dataset_test.head(5)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ume8Dxcx9zBk"},"source":["# def standardize_data(dataset, exclude_columns,):\n","#   dataset = dataset.copy(deep=True)\n","#   dataset_cols = list(set(dataset.columns.values) - set(exclude_columns))\n","#   for column_name in dataset_cols:\n","#     column = dataset[column_name].to_numpy()\n","#     # print(column, column_name)\n","#     nan_indexes = np.isnan(column)\n","#     real_indexes = ~np.isnan(column)\n","\n","#     n = real_indexes.astype(int).sum()\n","#     real_column = column[real_indexes]\n","\n","#     real_column_mean = real_column.sum() / n # or .mean()  \n","#     real_column_var = (np.sum([(column[i]-real_column_mean)**2 if item else 0 for i, item in enumerate(list(real_indexes))]) / n)**(1/2)\n","#     std_column = np.array([(column[i]-real_column_mean) / real_column_var if item else 0 for i, item in enumerate(list(real_indexes))])\n","    \n","#     if np.isnan(std_column).any():\n","#       std_column = np.zeros_like(std_column)\n","    \n","#     dataset.loc[:, column_name] = std_column \n","#   return dataset\n","\n","# # dataset_train = standardize_data(dataset_train, ['SalePrice'])\n","# # dataset_test = standardize_data(dataset_test, ['SalePrice'])\n","# # dataset_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pa_MKmOkHULW"},"source":["# dataset_test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZtEV6Sy8g_d"},"source":["# fig, ax = plt.subplots(figsize=(40,40))\n","# corr = dataset_train.corr()# plot the heatmap\n","# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True), ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRlsRVGNH_PK","executionInfo":{"status":"ok","timestamp":1638929150925,"user_tz":-180,"elapsed":291,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"a2fb49b0-74b2-4480-d735-58a6c94a483a"},"source":["class MultiLayerModel(nn.Module):\n","  def __init__(self, \n","    in_features=None,\n","    out_features=None,\n","    dropout=0.1\n","               \n","    ):\n","    super(MultiLayerModel, self).__init__()\n","    self.lin_1 = nn.Linear(in_features=in_features, out_features=in_features*2, ) \n","    self.lin_2 = nn.Linear(in_features=in_features*2, out_features=in_features*2) \n","    self.lin_3 = nn.Linear(in_features=in_features*2, out_features=out_features) \n","\n","    self.relu = nn.ReLU()\n","    self.tanh = nn.Tanh()\n","    self.drop_1 = nn.Dropout(p=dropout)\n","    self.drop_2 = nn.Dropout(p=dropout)\n","\n","  def forward(self, x):\n","    x = self.drop_1(self.tanh(self.lin_1(x)))\n","    x = self.drop_2(self.relu(self.lin_2(x)))\n","    x = self.relu(self.lin_3(x))\n","    return x\n","\n","class RMSLELogLoss(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.mse = nn.MSELoss()\n","      \n","  def forward(self, pred, actual):\n","    clipped_preds = torch.clamp(pred, 1, float('inf'))\n","    return torch.sqrt(self.mse(torch.log(clipped_preds), torch.log(actual)))\n","\n","model = MultiLayerModel(in_features=3, out_features=1)\n","print(model(torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.11, 0.12, 0.13]], dtype=torch.float32)))\n","predicts = model(torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.11, 0.12, 0.13]], dtype=torch.float32))\n","loss = RMSLELogLoss()(predicts, torch.tensor([[0.12], [0.15], [0.18], [0.19]]))\n","print(loss)\n","loss.backward()"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.],\n","        [0.],\n","        [0.],\n","        [0.]], grad_fn=<ReluBackward0>)\n","tensor(1.8570, grad_fn=<SqrtBackward0>)\n"]}]},{"cell_type":"code","metadata":{"id":"lsAkQ4upICSv"},"source":["class CustomPandasDataset(Dataset):\n","  def __init__(self, \n","                dataset=None,\n","                target_columns=None,\n","                infer=False\n","      ):\n","      self.dataset = dataset\n","      self.target_columns = target_columns\n","      self.infer = infer\n","\n","  def __len__(self):\n","      return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","      if self.infer:\n","        data_row = self.dataset.iloc[idx]\n","        target = torch.tensor([0], dtype=torch.float32)\n","        features = torch.tensor(data_row.values, dtype=torch.float32)\n","        return features, target \n","      else:\n","        data_row = self.dataset.iloc[idx]\n","        target = torch.tensor(data_row[self.target_columns].values, dtype=torch.float32)\n","        data_row = data_row.drop(labels=self.target_columns)\n","        features = torch.tensor(data_row.values, dtype=torch.float32)\n","\n","        return features, target \n","\n","temp_dataset = CustomPandasDataset(\n","    dataset=dataset,\n","    target_columns=['SalePrice'],\n","    # infer=True\n",")\n","\n","temp_dataloader = DataLoader(\n","    dataset=temp_dataset, \n","    batch_size=16, \n","    shuffle=True\n",")\n","\n","next(iter(temp_dataloader))\n","# temp_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlhyaX4e7Z6b"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"czZBhu147Z39"},"source":["# Cross validation class"]},{"cell_type":"code","metadata":{"id":"flqSnKEmIQ9p","executionInfo":{"status":"ok","timestamp":1638929152852,"user_tz":-180,"elapsed":362,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["class Trainer:\n","  def __init__(self, \n","    model=None, \n","    dataset=None,\n","    optimizer_class=None,\n","    loss_func=None,\n","    estimate_func=None,\n","    batch_size=None,\n","    model_hyperparams=None,\n","    model_class=None\n","    ): \n","    self.model = None\n","    self.dataset = dataset\n","    self.optimizer_class = optimizer_class\n","    self.optimizer = None\n","    self.loss_func = loss_func\n","    self.estimate_func = estimate_func\n","    # self.folds_models = []\n","    self.folds_avg_train_loss = []\n","    self.folds_avg_test_loss = []\n","    self.batch_size = batch_size\n","\n","    self.model_class = model_class\n","    self.model_hyperparams = model_hyperparams\n","\n","  def train(self, \n","            epochs=1,\n","            lr=5,\n","            k_folds=5,\n","            weight_decay=0\n","    ):\n","    # source: https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/\n","    \n","    kfold = KFold(n_splits=k_folds, shuffle=True)\n","\n","    for fold_num, (train_ids, test_ids) in enumerate(kfold.split(self.dataset)):\n","      print(f\"Fold {fold_num}\")\n","\n","      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","      \n","      trainloader = DataLoader(\n","        self.dataset, \n","        batch_size=self.batch_size,\n","        sampler=train_subsampler\n","      )\n","      \n","      testloader = DataLoader(\n","        self.dataset,\n","        batch_size=self.batch_size,\n","        sampler=test_subsampler\n","      )\n","      \n","      self.model = self.model_class(**self.model_hyperparams)\n","      self.optimizer = self.optimizer_class(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n","      \n","      fold_train = 0\n","      fold_test = 0\n","      for epoch in range(epochs):\n","        train_loss = 0\n","        test_loss = 0\n","        \n","        # train\n","        self.model.train()\n","        for features, targets in trainloader:\n","          self.optimizer.zero_grad()\n","\n","          predicts = self.model(features)\n","          loss = self.loss_func(predicts, targets)\n","          loss.backward()\n","          self.optimizer.step()\n","\n","          # train metric\n","          with torch.no_grad():\n","            train_loss += self.estimate_func(predicts, targets)\n","        \n","        # test metric\n","        self.model.eval()\n","        with torch.no_grad():\n","          for features, targets in testloader:\n","            predicts = self.model(features)\n","            test_loss += self.estimate_func(predicts, targets)\n","        \n","        avg_train = train_loss/len(trainloader)\n","        avg_test = test_loss/len(testloader)\n","        fold_train = avg_train\n","        fold_test  = avg_test \n","        print(f\"Fold {fold_num} Epoch {epoch}: train: {avg_train} test: {avg_test}\")\n","\n","      last_fold_train = fold_train #/ epochs\n","      last_fold_test = fold_test #/ epochs\n","\n","      print(f\"Fold {fold_num} ---- Train: {last_fold_train} Test: {last_fold_test} ---- \")\n","\n","      self.folds_avg_train_loss.append(last_fold_train)\n","      self.folds_avg_test_loss.append(last_fold_test)\n","\n","      # del self.model\n","      # del self.optimizer\n","\n","    print(\"Statistics:\")\n","    for i in range(k_folds):\n","      train_l = self.folds_avg_train_loss[i]\n","      test_l = self.folds_avg_test_loss[i]\n","      print(f\"Fold {i}: Train: {train_l} Test: {test_l} diff {abs(train_l - test_l)}\")\n","\n","  def train_pred(self, \n","    epochs=1,\n","    lr=5,\n","    weight_decay=0\n","  ):\n","\n","    trainloader = DataLoader(\n","      dataset=self.dataset, \n","      batch_size=self.batch_size\n","    )\n","\n","    self.model = self.model_class(**self.model_hyperparams)\n","    self.optimizer = self.optimizer_class(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    for epoch in range(epochs):\n","      train_loss = 0\n","      test_loss = 0\n","\n","      # train\n","      for features, targets in trainloader:\n","        self.optimizer.zero_grad()\n","\n","        predicts = self.model(features)\n","        loss = self.loss_func(predicts, targets)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # train metric\n","        with torch.no_grad():\n","          train_loss += self.estimate_func(predicts, targets)\n","\n","      avg_train = train_loss/len(trainloader) \n","      print(f\"Epoch {epoch}: train: {avg_train}\")\n","      \n","\n","target_columns = ['SalePrice']\n","batch_size = 64\n","\n","model_hyperparams = {\n","    'in_features': dataset.shape[1]-1,\n","    'out_features': 1,\n","    'dropout': 0.1\n","}\n","\n","custom_dataset = CustomPandasDataset(\n","    dataset, \n","    target_columns=target_columns,\n",")\n","\n","optimizer_class = torch.optim.Adam\n","loss_func = nn.MSELoss()\n","trainer = Trainer(\n","  dataset=custom_dataset,\n","  optimizer_class=optimizer_class,\n","  loss_func=loss_func, \n","  estimate_func=RMSLELogLoss(),\n","  batch_size=batch_size,\n","  model_hyperparams=model_hyperparams,\n","  model_class=MultiLayerModel\n",")\n","# trainer.train(epochs=60, lr=7,)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yJ7sSkdNUdH","executionInfo":{"status":"ok","timestamp":1638929606513,"user_tz":-180,"elapsed":451960,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"534cf543-f379-4ceb-b1cd-60b6eb039490"},"source":["trainer.train(epochs=40, lr=0.1,)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 0\n","Fold 0 Epoch 0: train: 1.961267352104187 test: 0.4489278793334961\n","Fold 0 Epoch 1: train: 0.4511357247829437 test: 0.3646053373813629\n","Fold 0 Epoch 2: train: 0.377147912979126 test: 0.32036206126213074\n","Fold 0 Epoch 3: train: 0.3192921578884125 test: 0.26087042689323425\n","Fold 0 Epoch 4: train: 0.27587050199508667 test: 0.20969028770923615\n","Fold 0 Epoch 5: train: 0.21530067920684814 test: 0.16231735050678253\n","Fold 0 Epoch 6: train: 0.1949654072523117 test: 0.1623096913099289\n","Fold 0 Epoch 7: train: 0.17799094319343567 test: 0.154078871011734\n","Fold 0 Epoch 8: train: 0.17288967967033386 test: 0.13750994205474854\n","Fold 0 Epoch 9: train: 0.17075566947460175 test: 0.13945816457271576\n","Fold 0 Epoch 10: train: 0.16339881718158722 test: 0.14571936428546906\n","Fold 0 Epoch 11: train: 0.16677427291870117 test: 0.13728797435760498\n","Fold 0 Epoch 12: train: 0.16572940349578857 test: 0.14969868957996368\n","Fold 0 Epoch 13: train: 0.16926315426826477 test: 0.15407638251781464\n","Fold 0 Epoch 14: train: 0.17426273226737976 test: 0.14451977610588074\n","Fold 0 Epoch 15: train: 0.16369344294071198 test: 0.1372208148241043\n","Fold 0 Epoch 16: train: 0.16163456439971924 test: 0.14076808094978333\n","Fold 0 Epoch 17: train: 0.15983714163303375 test: 0.1693698614835739\n","Fold 0 Epoch 18: train: 0.17152659595012665 test: 0.1540309637784958\n","Fold 0 Epoch 19: train: 0.15723378956317902 test: 0.13994145393371582\n","Fold 0 Epoch 20: train: 0.15022337436676025 test: 0.14089208841323853\n","Fold 0 Epoch 21: train: 0.15999580919742584 test: 0.1342075914144516\n","Fold 0 Epoch 22: train: 0.14994163811206818 test: 0.14389406144618988\n","Fold 0 Epoch 23: train: 0.15174506604671478 test: 0.15369132161140442\n","Fold 0 Epoch 24: train: 0.160068541765213 test: 0.1372717320919037\n","Fold 0 Epoch 25: train: 0.14546002447605133 test: 0.1304483860731125\n","Fold 0 Epoch 26: train: 0.14466360211372375 test: 0.13798291981220245\n","Fold 0 Epoch 27: train: 0.14706513285636902 test: 0.1420573890209198\n","Fold 0 Epoch 28: train: 0.1464993804693222 test: 0.14965933561325073\n","Fold 0 Epoch 29: train: 0.15769493579864502 test: 0.13337191939353943\n","Fold 0 Epoch 30: train: 0.14542840421199799 test: 0.1319439709186554\n","Fold 0 Epoch 31: train: 0.1422501504421234 test: 0.14175960421562195\n","Fold 0 Epoch 32: train: 0.13484697043895721 test: 0.1297302544116974\n","Fold 0 Epoch 33: train: 0.13981829583644867 test: 0.132839173078537\n","Fold 0 Epoch 34: train: 0.14557142555713654 test: 0.12832723557949066\n","Fold 0 Epoch 35: train: 0.1431693285703659 test: 0.1460006684064865\n","Fold 0 Epoch 36: train: 0.1547994464635849 test: 0.13880419731140137\n","Fold 0 Epoch 37: train: 0.14828704297542572 test: 0.12935514748096466\n","Fold 0 Epoch 38: train: 0.14697594940662384 test: 0.15144933760166168\n","Fold 0 Epoch 39: train: 0.139125794172287 test: 0.1361776739358902\n","Fold 0 ---- Train: 0.139125794172287 Test: 0.1361776739358902 ---- \n","Fold 1\n","Fold 1 Epoch 0: train: 2.037081480026245 test: 0.36882323026657104\n","Fold 1 Epoch 1: train: 0.4107811450958252 test: 0.34367942810058594\n","Fold 1 Epoch 2: train: 0.3764951527118683 test: 0.3108065724372864\n","Fold 1 Epoch 3: train: 0.3264298141002655 test: 0.25436821579933167\n","Fold 1 Epoch 4: train: 0.25723496079444885 test: 0.20109610259532928\n","Fold 1 Epoch 5: train: 0.20663030445575714 test: 0.17419809103012085\n","Fold 1 Epoch 6: train: 0.19110560417175293 test: 0.14548003673553467\n","Fold 1 Epoch 7: train: 0.16722524166107178 test: 0.14484882354736328\n","Fold 1 Epoch 8: train: 0.16326099634170532 test: 0.1469060182571411\n","Fold 1 Epoch 9: train: 0.17459632456302643 test: 0.13208840787410736\n","Fold 1 Epoch 10: train: 0.17321687936782837 test: 0.1465739756822586\n","Fold 1 Epoch 11: train: 0.17764723300933838 test: 0.15178687870502472\n","Fold 1 Epoch 12: train: 0.16789233684539795 test: 0.1316044181585312\n","Fold 1 Epoch 13: train: 0.1584254503250122 test: 0.1365385502576828\n","Fold 1 Epoch 14: train: 0.1578504741191864 test: 0.13998082280158997\n","Fold 1 Epoch 15: train: 0.16115307807922363 test: 0.1331232339143753\n","Fold 1 Epoch 16: train: 0.16172198951244354 test: 0.13200591504573822\n","Fold 1 Epoch 17: train: 0.14974507689476013 test: 0.13858562707901\n","Fold 1 Epoch 18: train: 0.15267419815063477 test: 0.12852825224399567\n","Fold 1 Epoch 19: train: 0.15247592329978943 test: 0.1343299150466919\n","Fold 1 Epoch 20: train: 0.15014629065990448 test: 0.1687951683998108\n","Fold 1 Epoch 21: train: 0.15822184085845947 test: 0.1315840482711792\n","Fold 1 Epoch 22: train: 0.14784638583660126 test: 0.1416558176279068\n","Fold 1 Epoch 23: train: 0.14349178969860077 test: 0.14651985466480255\n","Fold 1 Epoch 24: train: 0.1581273376941681 test: 0.13370785117149353\n","Fold 1 Epoch 25: train: 0.14314302802085876 test: 0.13120441138744354\n","Fold 1 Epoch 26: train: 0.14109019935131073 test: 0.13761191070079803\n","Fold 1 Epoch 27: train: 0.14102521538734436 test: 0.13597187399864197\n","Fold 1 Epoch 28: train: 0.14343106746673584 test: 0.13303357362747192\n","Fold 1 Epoch 29: train: 0.1406160444021225 test: 0.12100373208522797\n","Fold 1 Epoch 30: train: 0.13353657722473145 test: 0.12608233094215393\n","Fold 1 Epoch 31: train: 0.14377041161060333 test: 0.12715566158294678\n","Fold 1 Epoch 32: train: 0.14509843289852142 test: 0.13448871672153473\n","Fold 1 Epoch 33: train: 0.13910870254039764 test: 0.12382832914590836\n","Fold 1 Epoch 34: train: 0.1424374133348465 test: 0.19451209902763367\n","Fold 1 Epoch 35: train: 0.16164691746234894 test: 0.13203191757202148\n","Fold 1 Epoch 36: train: 0.13545164465904236 test: 0.13617980480194092\n","Fold 1 Epoch 37: train: 0.13483648002147675 test: 0.13364042341709137\n","Fold 1 Epoch 38: train: 0.15503361821174622 test: 0.1360207051038742\n","Fold 1 Epoch 39: train: 0.14031510055065155 test: 0.13908161222934723\n","Fold 1 ---- Train: 0.14031510055065155 Test: 0.13908161222934723 ---- \n","Fold 2\n","Fold 2 Epoch 0: train: 2.033339738845825 test: 0.47166872024536133\n","Fold 2 Epoch 1: train: 0.4033757448196411 test: 0.4714023470878601\n","Fold 2 Epoch 2: train: 0.3849961757659912 test: 0.434151828289032\n","Fold 2 Epoch 3: train: 0.36952751874923706 test: 0.3778149485588074\n","Fold 2 Epoch 4: train: 0.34884217381477356 test: 0.3855191767215729\n","Fold 2 Epoch 5: train: 0.3415854573249817 test: 0.3461301028728485\n","Fold 2 Epoch 6: train: 0.31251001358032227 test: 0.30475661158561707\n","Fold 2 Epoch 7: train: 0.2775748670101166 test: 0.31614014506340027\n","Fold 2 Epoch 8: train: 0.24892161786556244 test: 0.24237561225891113\n","Fold 2 Epoch 9: train: 0.23613959550857544 test: 0.24644887447357178\n","Fold 2 Epoch 10: train: 0.2368711233139038 test: 0.2048722505569458\n","Fold 2 Epoch 11: train: 0.22273103892803192 test: 0.23879270255565643\n","Fold 2 Epoch 12: train: 0.2207183837890625 test: 0.2044202834367752\n","Fold 2 Epoch 13: train: 0.2328539341688156 test: 0.2376711368560791\n","Fold 2 Epoch 14: train: 0.20695458352565765 test: 0.19378434121608734\n","Fold 2 Epoch 15: train: 0.20736131072044373 test: 0.19224169850349426\n","Fold 2 Epoch 16: train: 0.2002006322145462 test: 0.18772786855697632\n","Fold 2 Epoch 17: train: 0.2071821689605713 test: 0.21359999477863312\n","Fold 2 Epoch 18: train: 0.20479579269886017 test: 0.17742595076560974\n","Fold 2 Epoch 19: train: 0.20342402160167694 test: 0.18104040622711182\n","Fold 2 Epoch 20: train: 0.18390172719955444 test: 0.1739843785762787\n","Fold 2 Epoch 21: train: 0.17501015961170197 test: 0.16313573718070984\n","Fold 2 Epoch 22: train: 0.1781170517206192 test: 0.16194124519824982\n","Fold 2 Epoch 23: train: 0.17006659507751465 test: 0.16685184836387634\n","Fold 2 Epoch 24: train: 0.1708778440952301 test: 0.15817192196846008\n","Fold 2 Epoch 25: train: 0.1719871610403061 test: 0.15418583154678345\n","Fold 2 Epoch 26: train: 0.16603851318359375 test: 0.1564197689294815\n","Fold 2 Epoch 27: train: 0.17451508343219757 test: 0.2005990743637085\n","Fold 2 Epoch 28: train: 0.1896335780620575 test: 0.17526142299175262\n","Fold 2 Epoch 29: train: 0.17580732703208923 test: 0.20184683799743652\n","Fold 2 Epoch 30: train: 0.17360012233257294 test: 0.19217316806316376\n","Fold 2 Epoch 31: train: 0.19143855571746826 test: 0.15269571542739868\n","Fold 2 Epoch 32: train: 0.17252589762210846 test: 0.16564173996448517\n","Fold 2 Epoch 33: train: 0.17493340373039246 test: 0.16506831347942352\n","Fold 2 Epoch 34: train: 0.17173238098621368 test: 0.16163566708564758\n","Fold 2 Epoch 35: train: 0.17253799736499786 test: 0.15147802233695984\n","Fold 2 Epoch 36: train: 0.1633927822113037 test: 0.15650585293769836\n","Fold 2 Epoch 37: train: 0.1629919558763504 test: 0.155899316072464\n","Fold 2 Epoch 38: train: 0.174235537648201 test: 0.17794683575630188\n","Fold 2 Epoch 39: train: 0.17855794727802277 test: 0.1604931503534317\n","Fold 2 ---- Train: 0.17855794727802277 Test: 0.1604931503534317 ---- \n","Fold 3\n","Fold 3 Epoch 0: train: 1.9398647546768188 test: 0.4128986895084381\n","Fold 3 Epoch 1: train: 0.4219995439052582 test: 0.37038397789001465\n","Fold 3 Epoch 2: train: 0.37393540143966675 test: 0.34193846583366394\n","Fold 3 Epoch 3: train: 0.3194815218448639 test: 0.29065531492233276\n","Fold 3 Epoch 4: train: 0.26295366883277893 test: 0.23105666041374207\n","Fold 3 Epoch 5: train: 0.2027166187763214 test: 0.18165135383605957\n","Fold 3 Epoch 6: train: 0.1681300550699234 test: 0.16262075304985046\n","Fold 3 Epoch 7: train: 0.17148007452487946 test: 0.191287562251091\n","Fold 3 Epoch 8: train: 0.1725194901227951 test: 0.17853118479251862\n","Fold 3 Epoch 9: train: 0.17021052539348602 test: 0.14323215186595917\n","Fold 3 Epoch 10: train: 0.1527576744556427 test: 0.14607159793376923\n","Fold 3 Epoch 11: train: 0.15362823009490967 test: 0.1398308128118515\n","Fold 3 Epoch 12: train: 0.15491466224193573 test: 0.13504038751125336\n","Fold 3 Epoch 13: train: 0.15791404247283936 test: 0.1332044154405594\n","Fold 3 Epoch 14: train: 0.1495872288942337 test: 0.14814679324626923\n","Fold 3 Epoch 15: train: 0.15883980691432953 test: 0.14032959938049316\n","Fold 3 Epoch 16: train: 0.14679312705993652 test: 0.1340579092502594\n","Fold 3 Epoch 17: train: 0.1445043534040451 test: 0.14016909897327423\n","Fold 3 Epoch 18: train: 0.15679162740707397 test: 0.16635289788246155\n","Fold 3 Epoch 19: train: 0.1454998254776001 test: 0.13763107359409332\n","Fold 3 Epoch 20: train: 0.14294400811195374 test: 0.1392911970615387\n","Fold 3 Epoch 21: train: 0.1451849490404129 test: 0.1309618353843689\n","Fold 3 Epoch 22: train: 0.1495642215013504 test: 0.1373133808374405\n","Fold 3 Epoch 23: train: 0.15088100731372833 test: 0.13214117288589478\n","Fold 3 Epoch 24: train: 0.14211201667785645 test: 0.13935725390911102\n","Fold 3 Epoch 25: train: 0.1437651664018631 test: 0.13794216513633728\n","Fold 3 Epoch 26: train: 0.14332354068756104 test: 0.16135476529598236\n","Fold 3 Epoch 27: train: 0.14748039841651917 test: 0.1345803588628769\n","Fold 3 Epoch 28: train: 0.14057186245918274 test: 0.13006404042243958\n","Fold 3 Epoch 29: train: 0.1316617727279663 test: 0.12628577649593353\n","Fold 3 Epoch 30: train: 0.13554725050926208 test: 0.12383618205785751\n","Fold 3 Epoch 31: train: 0.1313735693693161 test: 0.13502728939056396\n","Fold 3 Epoch 32: train: 0.13552796840667725 test: 0.125715434551239\n","Fold 3 Epoch 33: train: 0.12701967358589172 test: 0.130549818277359\n","Fold 3 Epoch 34: train: 0.129952073097229 test: 0.14711502194404602\n","Fold 3 Epoch 35: train: 0.13965444266796112 test: 0.1278885304927826\n","Fold 3 Epoch 36: train: 0.13162370026111603 test: 0.13564804196357727\n","Fold 3 Epoch 37: train: 0.1271512806415558 test: 0.1398363560438156\n","Fold 3 Epoch 38: train: 0.12563535571098328 test: 0.12720082700252533\n","Fold 3 Epoch 39: train: 0.12642547488212585 test: 0.1312132477760315\n","Fold 3 ---- Train: 0.12642547488212585 Test: 0.1312132477760315 ---- \n","Fold 4\n","Fold 4 Epoch 0: train: 2.0786919593811035 test: 0.4008975923061371\n","Fold 4 Epoch 1: train: 0.3131387233734131 test: 0.22176530957221985\n","Fold 4 Epoch 2: train: 0.18511542677879333 test: 0.18284979462623596\n","Fold 4 Epoch 3: train: 0.15436886250972748 test: 0.16888155043125153\n","Fold 4 Epoch 4: train: 0.14152343571186066 test: 0.13641571998596191\n","Fold 4 Epoch 5: train: 0.13224661350250244 test: 0.14202716946601868\n","Fold 4 Epoch 6: train: 0.13242225348949432 test: 0.13822565972805023\n","Fold 4 Epoch 7: train: 0.12613382935523987 test: 0.12482789903879166\n","Fold 4 Epoch 8: train: 0.12874089181423187 test: 0.14053121209144592\n","Fold 4 Epoch 9: train: 0.12950719892978668 test: 0.13309797644615173\n","Fold 4 Epoch 10: train: 0.12896257638931274 test: 0.1361262947320938\n","Fold 4 Epoch 11: train: 0.12479611486196518 test: 0.15278342366218567\n","Fold 4 Epoch 12: train: 0.12868209183216095 test: 0.13285361230373383\n","Fold 4 Epoch 13: train: 0.12227901816368103 test: 0.1244872435927391\n","Fold 4 Epoch 14: train: 0.12698307633399963 test: 0.12902958691120148\n","Fold 4 Epoch 15: train: 0.12463659793138504 test: 0.14044877886772156\n","Fold 4 Epoch 16: train: 0.12231037765741348 test: 0.12391938269138336\n","Fold 4 Epoch 17: train: 0.12473315745592117 test: 0.12760287523269653\n","Fold 4 Epoch 18: train: 0.11843901127576828 test: 0.13403353095054626\n","Fold 4 Epoch 19: train: 0.11510606855154037 test: 0.13500550389289856\n","Fold 4 Epoch 20: train: 0.12349805980920792 test: 0.12478693574666977\n","Fold 4 Epoch 21: train: 0.11597155034542084 test: 0.13136741518974304\n","Fold 4 Epoch 22: train: 0.12130159139633179 test: 0.13191896677017212\n","Fold 4 Epoch 23: train: 0.11951453983783722 test: 0.12383922189474106\n","Fold 4 Epoch 24: train: 0.11696934700012207 test: 0.1269385814666748\n","Fold 4 Epoch 25: train: 0.1295849084854126 test: 0.12815453112125397\n","Fold 4 Epoch 26: train: 0.12389741092920303 test: 0.13356590270996094\n","Fold 4 Epoch 27: train: 0.1184830442070961 test: 0.13349422812461853\n","Fold 4 Epoch 28: train: 0.12215196341276169 test: 0.13005629181861877\n","Fold 4 Epoch 29: train: 0.1168009489774704 test: 0.13181936740875244\n","Fold 4 Epoch 30: train: 0.12118321657180786 test: 0.12842699885368347\n","Fold 4 Epoch 31: train: 0.11821148544549942 test: 0.13089227676391602\n","Fold 4 Epoch 32: train: 0.11896628886461258 test: 0.1259864717721939\n","Fold 4 Epoch 33: train: 0.12057486176490784 test: 0.1583329141139984\n","Fold 4 Epoch 34: train: 0.11743050068616867 test: 0.17102620005607605\n","Fold 4 Epoch 35: train: 0.11941356211900711 test: 0.12144060432910919\n","Fold 4 Epoch 36: train: 0.11494207382202148 test: 0.1287248432636261\n","Fold 4 Epoch 37: train: 0.11298685520887375 test: 0.13169808685779572\n","Fold 4 Epoch 38: train: 0.10910017788410187 test: 0.12951256334781647\n","Fold 4 Epoch 39: train: 0.11196140199899673 test: 0.1351369321346283\n","Fold 4 ---- Train: 0.11196140199899673 Test: 0.1351369321346283 ---- \n","Statistics:\n","Fold 0: Train: 0.139125794172287 Test: 0.1361776739358902 diff 0.0029481202363967896\n","Fold 1: Train: 0.14031510055065155 Test: 0.13908161222934723 diff 0.0012334883213043213\n","Fold 2: Train: 0.17855794727802277 Test: 0.1604931503534317 diff 0.018064796924591064\n","Fold 3: Train: 0.12642547488212585 Test: 0.1312132477760315 diff 0.00478777289390564\n","Fold 4: Train: 0.11196140199899673 Test: 0.1351369321346283 diff 0.02317553013563156\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pO_tvGoIivC","executionInfo":{"status":"ok","timestamp":1638925075221,"user_tz":-180,"elapsed":62319,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"422aa83d-046f-46ec-f65e-abd7c6c4cef2"},"source":["trainer.train_pred(epochs=30, lr=0.1, weight_decay=0)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train: 1.9655839204788208\n","Epoch 1: train: 0.4111209213733673\n","Epoch 2: train: 0.3737131953239441\n","Epoch 3: train: 0.3248210549354553\n","Epoch 4: train: 0.26499512791633606\n","Epoch 5: train: 0.21358956396579742\n","Epoch 6: train: 0.1817256659269333\n","Epoch 7: train: 0.16776414215564728\n","Epoch 8: train: 0.1619671732187271\n","Epoch 9: train: 0.16288045048713684\n","Epoch 10: train: 0.1643836349248886\n","Epoch 11: train: 0.15846772491931915\n","Epoch 12: train: 0.1555902510881424\n","Epoch 13: train: 0.15280218422412872\n","Epoch 14: train: 0.14985325932502747\n","Epoch 15: train: 0.15053148567676544\n","Epoch 16: train: 0.15302424132823944\n","Epoch 17: train: 0.15282587707042694\n","Epoch 18: train: 0.151051327586174\n","Epoch 19: train: 0.1490776687860489\n","Epoch 20: train: 0.15321826934814453\n","Epoch 21: train: 0.15334300696849823\n","Epoch 22: train: 0.15836305916309357\n","Epoch 23: train: 0.1541217565536499\n","Epoch 24: train: 0.16013631224632263\n","Epoch 25: train: 0.14268113672733307\n","Epoch 26: train: 0.15473772585391998\n","Epoch 27: train: 0.14979392290115356\n","Epoch 28: train: 0.1476757973432541\n","Epoch 29: train: 0.14557811617851257\n"]}]},{"cell_type":"code","metadata":{"id":"ku2GhgFS5JD7"},"source":["# model=MultiLayerModel(\n","#     features=features_size, \n","#     targets=targets_size\n","# )\n","# model = nn.Linear(in_features=features_size, out_features=targets_size)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","# loss_func = nn.MSELoss()\n","# acc_func = RMSLELogLoss()\n","\n","# for ep in range(15):\n","#   total_loss = 0\n","#   total_elements = 0\n","#   for fea, tar in train_dataloader:\n","#     pred = model(fea)\n","#     loss = loss_func(pred, tar)\n","#     loss.backward()\n","#     optimizer.step()\n","#     optimizer.zero_grad()\n","#     with torch.no_grad():\n","#       total_loss += acc_func(pred, tar)\n","#       total_elements += len(tar)\n","#   print(total_loss/total_elements)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVRe5OoIGpJg"},"source":["## submit"]},{"cell_type":"code","metadata":{"id":"Xe7JhkkpGrxU","executionInfo":{"status":"ok","timestamp":1638929623343,"user_tz":-180,"elapsed":281,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["dataset_submit = pd.read_csv(\"./data/HousePricesKaggle/test.csv\")\n","\n","\n","drop_columns = 'Id'.split()\n","dataset_submit = dataset_submit.drop(columns=drop_columns)\n","\n","select_columns = dataset_submit.dtypes[dataset_submit.dtypes != 'object'].index.tolist()\n","# select_columns = list(set(select_columns) - set(['SalePrice']))\n","\n","dataset_submit[select_columns] = dataset_submit[select_columns].apply(lambda x: (x - x.mean()) / (x.std()))\n","target_columns = ['SalePrice']\n","dataset_submit[select_columns] = dataset_submit[select_columns].fillna(dataset_submit[select_columns].mean() / dataset_submit[select_columns].std())\n","\n","OH_cols_submit = pd.DataFrame(OH_encoder.transform(dataset_submit[object_columns]))\n","\n","OH_cols_submit.index = dataset_submit.index\n","num_X_submit = dataset_submit.drop(object_columns, axis=1)\n","\n","dataset_submit = pd.concat([num_X_submit, OH_cols_submit], axis=1)\n","\n","batch_size = len(dataset_submit)\n","submit_dataset = CustomPandasDataset(\n","    dataset=dataset_submit,\n","    target_columns=target_columns,\n","    infer=True\n",")\n","\n","submit_dataloader = DataLoader(\n","    dataset=submit_dataset, \n","    batch_size=batch_size, \n","    shuffle=False,\n",")\n","\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx2bDBmfOPYG"},"source":["dataset_submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27phUdZ4H6eG","executionInfo":{"status":"ok","timestamp":1638929626384,"user_tz":-180,"elapsed":814,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["with torch.no_grad():\n","  features, targets = next(iter(submit_dataloader))\n","  # print(features)\n","  trainer.model.eval()\n","  preds = trainer.model(features).numpy().reshape(-1)\n","  # print(preds)\n","\n","  test_data_subm = pd.read_csv(\"./data/HousePricesKaggle/test.csv\")\n","  test_data_subm = test_data_subm[['Id']]\n","  test_data_subm['SalePrice'] = preds\n","\n","  test_data_subm.to_csv('./data/HousePricesKaggle/submission_my.csv', index=False)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yq5qA4JEMIQv","executionInfo":{"status":"ok","timestamp":1638281577437,"user_tz":-180,"elapsed":284,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"3710bf40-13ea-4d1f-fb28-25606c1c8edb"},"source":["np.array([[1], [2], [3]]).reshape(-1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":133}]}]}