{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultilayerPerceptron.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1xrZjaRkF94a4Qi-Zsf11VSB9gFIzlpdp","authorship_tag":"ABX9TyPP+FOTx4ixb/YXtwAsBVXd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dbdMWBrXxeBm","executionInfo":{"status":"ok","timestamp":1638996686275,"user_tz":-180,"elapsed":782,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["from IPython.display import clear_output\n","%cd /content/drive/MyDrive/collab_sandbox/learn_DL/dive_into_dl/\n","# !pip install colabcode\n","clear_output()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"vT19w8ieym6E","executionInfo":{"status":"ok","timestamp":1638996993221,"user_tz":-180,"elapsed":284,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["import pandas as pd\n","import torch\n","import numpy as np\n","from torch.utils import data\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import matplotlib.pyplot as plt\n","import seaborn.apionly as sns\n","from sklearn.model_selection import KFold\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"svVf77HIyvFO","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1638996693456,"user_tz":-180,"elapsed":757,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"729820e5-dfd2-42be-e3a6-382a97a399ed"},"source":["dataset = pd.read_csv(\"./data/HousePricesKaggle/train.csv\")\n","\n","drop_columns = 'Id'.split()\n","dataset = dataset.drop(columns=drop_columns)\n","\n","select_columns = dataset.dtypes[dataset.dtypes != 'object'].index.tolist()\n","select_columns = list(set(select_columns) - set(['SalePrice']))\n","\n","dataset[select_columns] = dataset[select_columns].apply(lambda x: (x - x.mean()) / (x.std()))\n","\n","dataset[select_columns] = dataset[select_columns].fillna(dataset[select_columns].mean() / dataset[select_columns].std())\n","\n","# encode to one hot https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe\n","OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","\n","object_columns = dataset.dtypes[dataset.dtypes == 'object'].index.tolist()\n","OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(dataset[object_columns]))\n","\n","OH_cols_train.index = dataset.index\n","num_X_train = dataset.drop(object_columns, axis=1)\n","\n","dataset = pd.concat([num_X_train, OH_cols_train], axis=1)\n","\n","dataset.head()\n"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MSSubClass</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>OverallQual</th>\n","      <th>OverallCond</th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>MasVnrArea</th>\n","      <th>BsmtFinSF1</th>\n","      <th>BsmtFinSF2</th>\n","      <th>BsmtUnfSF</th>\n","      <th>TotalBsmtSF</th>\n","      <th>1stFlrSF</th>\n","      <th>2ndFlrSF</th>\n","      <th>LowQualFinSF</th>\n","      <th>GrLivArea</th>\n","      <th>BsmtFullBath</th>\n","      <th>BsmtHalfBath</th>\n","      <th>FullBath</th>\n","      <th>HalfBath</th>\n","      <th>BedroomAbvGr</th>\n","      <th>KitchenAbvGr</th>\n","      <th>TotRmsAbvGrd</th>\n","      <th>Fireplaces</th>\n","      <th>GarageYrBlt</th>\n","      <th>GarageCars</th>\n","      <th>GarageArea</th>\n","      <th>WoodDeckSF</th>\n","      <th>OpenPorchSF</th>\n","      <th>EnclosedPorch</th>\n","      <th>3SsnPorch</th>\n","      <th>ScreenPorch</th>\n","      <th>PoolArea</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SalePrice</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>...</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","      <th>233</th>\n","      <th>234</th>\n","      <th>235</th>\n","      <th>236</th>\n","      <th>237</th>\n","      <th>238</th>\n","      <th>239</th>\n","      <th>240</th>\n","      <th>241</th>\n","      <th>242</th>\n","      <th>243</th>\n","      <th>244</th>\n","      <th>245</th>\n","      <th>246</th>\n","      <th>247</th>\n","      <th>248</th>\n","      <th>249</th>\n","      <th>250</th>\n","      <th>251</th>\n","      <th>252</th>\n","      <th>253</th>\n","      <th>254</th>\n","      <th>255</th>\n","      <th>256</th>\n","      <th>257</th>\n","      <th>258</th>\n","      <th>259</th>\n","      <th>260</th>\n","      <th>261</th>\n","      <th>262</th>\n","      <th>263</th>\n","      <th>264</th>\n","      <th>265</th>\n","      <th>266</th>\n","      <th>267</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.073350</td>\n","      <td>-0.207948</td>\n","      <td>-0.207071</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>1.050634</td>\n","      <td>0.878367</td>\n","      <td>0.509840</td>\n","      <td>0.575228</td>\n","      <td>-0.288554</td>\n","      <td>-0.944267</td>\n","      <td>-0.459145</td>\n","      <td>-0.793162</td>\n","      <td>1.161454</td>\n","      <td>-0.120201</td>\n","      <td>0.370207</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>0.911897</td>\n","      <td>-0.950901</td>\n","      <td>0.992066</td>\n","      <td>0.311618</td>\n","      <td>0.350880</td>\n","      <td>-0.751918</td>\n","      <td>0.216429</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-1.598563</td>\n","      <td>0.138730</td>\n","      <td>208500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.872264</td>\n","      <td>0.409724</td>\n","      <td>-0.091855</td>\n","      <td>-0.071812</td>\n","      <td>2.178881</td>\n","      <td>0.156680</td>\n","      <td>-0.429430</td>\n","      <td>-0.572637</td>\n","      <td>1.171591</td>\n","      <td>-0.288554</td>\n","      <td>-0.641008</td>\n","      <td>0.466305</td>\n","      <td>0.257052</td>\n","      <td>-0.794891</td>\n","      <td>-0.120201</td>\n","      <td>-0.482347</td>\n","      <td>-0.819684</td>\n","      <td>3.947457</td>\n","      <td>0.789470</td>\n","      <td>-0.761360</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>-0.318574</td>\n","      <td>0.600289</td>\n","      <td>-0.101506</td>\n","      <td>0.311618</td>\n","      <td>-0.060710</td>\n","      <td>1.625638</td>\n","      <td>-0.704242</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-0.488943</td>\n","      <td>-0.614228</td>\n","      <td>181500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.073350</td>\n","      <td>-0.084413</td>\n","      <td>0.073455</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>0.984415</td>\n","      <td>0.829930</td>\n","      <td>0.322063</td>\n","      <td>0.092875</td>\n","      <td>-0.288554</td>\n","      <td>-0.301540</td>\n","      <td>-0.313261</td>\n","      <td>-0.627611</td>\n","      <td>1.188943</td>\n","      <td>-0.120201</td>\n","      <td>0.514836</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>-0.318574</td>\n","      <td>0.600289</td>\n","      <td>0.911061</td>\n","      <td>0.311618</td>\n","      <td>0.631510</td>\n","      <td>-0.751918</td>\n","      <td>-0.070337</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>0.990552</td>\n","      <td>0.138730</td>\n","      <td>223500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.309753</td>\n","      <td>-0.413838</td>\n","      <td>-0.096864</td>\n","      <td>0.651256</td>\n","      <td>-0.517023</td>\n","      <td>-1.862993</td>\n","      <td>-0.720051</td>\n","      <td>-0.572637</td>\n","      <td>-0.499103</td>\n","      <td>-0.288554</td>\n","      <td>-0.061648</td>\n","      <td>-0.687089</td>\n","      <td>-0.521555</td>\n","      <td>0.936955</td>\n","      <td>-0.120201</td>\n","      <td>0.383528</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>-1.025689</td>\n","      <td>-0.761360</td>\n","      <td>0.163723</td>\n","      <td>-0.211381</td>\n","      <td>0.296662</td>\n","      <td>0.600289</td>\n","      <td>0.789553</td>\n","      <td>1.649742</td>\n","      <td>0.790533</td>\n","      <td>-0.751918</td>\n","      <td>-0.175988</td>\n","      <td>4.091122</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>-1.598563</td>\n","      <td>-1.367186</td>\n","      <td>140000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.073350</td>\n","      <td>0.574436</td>\n","      <td>0.375020</td>\n","      <td>1.374324</td>\n","      <td>-0.517023</td>\n","      <td>0.951306</td>\n","      <td>0.733056</td>\n","      <td>1.360357</td>\n","      <td>0.463410</td>\n","      <td>-0.288554</td>\n","      <td>-0.174805</td>\n","      <td>0.199611</td>\n","      <td>-0.045596</td>\n","      <td>1.617323</td>\n","      <td>-0.120201</td>\n","      <td>1.298881</td>\n","      <td>1.107431</td>\n","      <td>-0.240978</td>\n","      <td>0.789470</td>\n","      <td>1.227165</td>\n","      <td>1.389547</td>\n","      <td>-0.211381</td>\n","      <td>1.527133</td>\n","      <td>0.600289</td>\n","      <td>0.870558</td>\n","      <td>1.649742</td>\n","      <td>1.697903</td>\n","      <td>0.779930</td>\n","      <td>0.563567</td>\n","      <td>-0.359202</td>\n","      <td>-0.116299</td>\n","      <td>-0.270116</td>\n","      <td>-0.068668</td>\n","      <td>-0.087658</td>\n","      <td>2.100173</td>\n","      <td>0.138730</td>\n","      <td>250000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 305 columns</p>\n","</div>"],"text/plain":["   MSSubClass  LotFrontage   LotArea  OverallQual  ...  264  265  266  267\n","0    0.073350    -0.207948 -0.207071     0.651256  ...  0.0  0.0  1.0  0.0\n","1   -0.872264     0.409724 -0.091855    -0.071812  ...  0.0  0.0  1.0  0.0\n","2    0.073350    -0.084413  0.073455     0.651256  ...  0.0  0.0  1.0  0.0\n","3    0.309753    -0.413838 -0.096864     0.651256  ...  0.0  0.0  0.0  0.0\n","4    0.073350     0.574436  0.375020     1.374324  ...  0.0  0.0  1.0  0.0\n","\n","[5 rows x 305 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"z3629TUOq-_N"},"source":["# fig, ax = plt.subplots(figsize=(40,40))\n","# corr = dataset_train.corr()# plot the heatmap\n","# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True), ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3j48Kai1umU"},"source":["# def encode_categorical(dataset, columns, full_dataset):\n","#   dataset = dataset.copy(deep=True)\n","#   # print(dataset)\n","#   dataset_cols = dataset.columns.values\n","#   for column in columns:\n","#     if column in dataset_cols:\n","#       values_list = full_dataset[column].values.tolist()\n","#       list_columns = sorted(list(map(str, list(set(values_list)))))\n","#       list_columns_renamed = [f'{column}_{item}' for item in list_columns]\n","#       dict_columns = {item: i for (i, item) in enumerate(list_columns)}\n","      \n","#       # print(dataset[column].)\n","#       dataset_values = dataset[column].tolist()\n","#       for i, value in enumerate(dataset_values):\n","#         value = str(value)\n","\n","#         ohe_hot = [0 for _ in range(len(list_columns))]\n","#         if dict_columns.get(value):\n","#           ohe_hot[dict_columns.get(value)] = 1\n","\n","#         # slow operation!\n","#         dataset.loc[i, list_columns_renamed] = ohe_hot\n","#       dataset = dataset.drop(columns=[column],) \n","  \n","#   return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFlJlWaYGwZJ"},"source":["# dataset_train.dtypes[dataset_train.dtypes == 'object'].index.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAbktca95BeZ"},"source":["# categorical_labels = dataset_train.dtypes[dataset_train.dtypes == 'object'].index.tolist() #list(set('MSSubClass PavedDrive MSZoning Street\tAlley\tLotShape\tLandContour\tUtilities\tLotConfig\tLandSlope\tNeighborhood\tCondition1\tCondition2\tBldgType\tHouseStyle RoofStyle\tRoofMatl\tExterior1st\tExterior2nd\tMasVnrType ExterQual\tExterCond\tFoundation\tBsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1 BsmtFinType2 Heating\tHeatingQC\tCentralAir\tElectrical KitchenQual Functional FireplaceQu\tGarageType GarageFinish PoolQC\tFence\tMiscFeature SaleType\tSaleCondition GarageCond GarageQual GarageType GarageFinish\tGarageQual GarageCond'.split()))\n","# dataset_train = encode_categorical(dataset_train, categorical_labels, dataset)\n","# dataset_test = encode_categorical(dataset_test, categorical_labels, dataset)\n","# dataset_train.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbAsU_jq8f_4"},"source":["# len(dataset_train.columns) == len(dataset_test.columns), len(dataset_test.columns), len(dataset_train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRTuFNJGJ975"},"source":["# dataset_train.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdwDbNtw9Gqb"},"source":["# dataset_test.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ume8Dxcx9zBk"},"source":["# def standardize_data(dataset, exclude_columns,):\n","#   dataset = dataset.copy(deep=True)\n","#   dataset_cols = list(set(dataset.columns.values) - set(exclude_columns))\n","#   for column_name in dataset_cols:\n","#     column = dataset[column_name].to_numpy()\n","#     # print(column, column_name)\n","#     nan_indexes = np.isnan(column)\n","#     real_indexes = ~np.isnan(column)\n","\n","#     n = real_indexes.astype(int).sum()\n","#     real_column = column[real_indexes]\n","\n","#     real_column_mean = real_column.sum() / n # or .mean()  \n","#     real_column_var = (np.sum([(column[i]-real_column_mean)**2 if item else 0 for i, item in enumerate(list(real_indexes))]) / n)**(1/2)\n","#     std_column = np.array([(column[i]-real_column_mean) / real_column_var if item else 0 for i, item in enumerate(list(real_indexes))])\n","    \n","#     if np.isnan(std_column).any():\n","#       std_column = np.zeros_like(std_column)\n","    \n","#     dataset.loc[:, column_name] = std_column \n","#   return dataset\n","\n","# # dataset_train = standardize_data(dataset_train, ['SalePrice'])\n","# # dataset_test = standardize_data(dataset_test, ['SalePrice'])\n","# # dataset_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pa_MKmOkHULW"},"source":["# dataset_test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZtEV6Sy8g_d"},"source":["# fig, ax = plt.subplots(figsize=(40,40))\n","# corr = dataset_train.corr()# plot the heatmap\n","# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True), ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRlsRVGNH_PK","executionInfo":{"status":"ok","timestamp":1638998422724,"user_tz":-180,"elapsed":277,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"f505e307-3717-4ce2-e18b-7b72d1074049"},"source":["class MultiLayerModel(nn.Module):\n","  def __init__(self, \n","    in_features=None,\n","    out_features=None,\n","    dropout=0.1\n","               \n","    ):\n","    super(MultiLayerModel, self).__init__()\n","   \n","    self.lin_1 = nn.Linear(in_features=in_features, out_features=in_features, ) \n","    self.lin_2 = nn.Linear(in_features=in_features, out_features=in_features, ) \n","    self.lin_3 = nn.Linear(in_features=in_features, out_features=out_features, ) \n","\n","    self.relu = nn.ReLU()\n","    self.tanh = nn.Tanh()\n","    self.gelu = nn.GELU()\n","    self.selu = nn.SELU()\n","\n","    self.drop_1 = nn.Dropout(p=dropout)\n","    self.drop_2 = nn.Dropout(p=dropout)\n","\n","\n","    # self.norm = nn.LayerNorm()\n","\n","    self.eps = 0.0001\n","\n","  def normalize(self, x):\n","    return (x - x.mean(dim=0))/(x.std(dim=0)+self.eps)\n","\n","  def forward(self, x):\n","    x = self.drop_1(self.gelu(self.lin_1(x)))\n","    x = self.drop_1(self.selu(self.lin_2(x)))\n","    x = self.lin_3(x)\n","    return x\n","\n","class RMSLELogLoss(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.mse = nn.MSELoss()\n","      \n","  def forward(self, pred, actual):\n","    clipped_preds = torch.clamp(pred, 1, float('inf'))\n","    return torch.sqrt(self.mse(torch.log(clipped_preds), torch.log(actual)))\n","\n","model = MultiLayerModel(in_features=3, out_features=1)\n","print(model(torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.11, 0.12, 0.13]], dtype=torch.float32, )))\n","predicts = model(torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.11, 0.12, 0.13]], dtype=torch.float32, ))\n","loss = RMSLELogLoss()(predicts, torch.tensor([[0.12], [0.15], [0.18], [0.19]], ))\n","print(loss)\n","loss.backward()"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0491],\n","        [-0.1591],\n","        [ 0.2540],\n","        [-0.0641]], grad_fn=<AddmmBackward0>)\n","tensor(1.8570, grad_fn=<SqrtBackward0>)\n"]}]},{"cell_type":"code","metadata":{"id":"lsAkQ4upICSv"},"source":["class CustomPandasDataset(Dataset):\n","  def __init__(self, \n","                dataset=None,\n","                target_columns=None,\n","                infer=False\n","      ):\n","      self.dataset = dataset\n","      self.target_columns = target_columns\n","      self.infer = infer\n","      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","  def __len__(self):\n","      return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","      if self.infer:\n","        data_row = self.dataset.iloc[idx]\n","        target = torch.tensor([0], dtype=torch.float32, device=self.device)\n","        features = torch.tensor(data_row.values, dtype=torch.float32, device=self.device)\n","        return features, target \n","      else:\n","        data_row = self.dataset.iloc[idx]\n","        target = torch.tensor(data_row[self.target_columns].values, dtype=torch.float32, device=self.device)\n","        data_row = data_row.drop(labels=self.target_columns)\n","        features = torch.tensor(data_row.values, dtype=torch.float32, device=self.device)\n","\n","        return features, target \n","\n","temp_dataset = CustomPandasDataset(\n","    dataset=dataset,\n","    target_columns=['SalePrice'],\n","    # infer=True\n",")\n","\n","temp_dataloader = DataLoader(\n","    dataset=temp_dataset, \n","    batch_size=16, \n","    shuffle=True\n",")\n","\n","next(iter(temp_dataloader))\n","# temp_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlhyaX4e7Z6b"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"czZBhu147Z39"},"source":["# Cross validation class"]},{"cell_type":"code","metadata":{"id":"flqSnKEmIQ9p","executionInfo":{"status":"ok","timestamp":1638998425540,"user_tz":-180,"elapsed":274,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9fe99cc-e5fc-4625-d0b6-3d9cb99f0927"},"source":["class Trainer:\n","  def __init__(self, \n","    model=None, \n","    dataset=None,\n","    optimizer_class=None,\n","    loss_func=None,\n","    estimate_func=None,\n","    batch_size=None,\n","    model_hyperparams=None,\n","    model_class=None\n","    ): \n","    self.model = None\n","    self.dataset = dataset\n","    self.optimizer_class = optimizer_class\n","    self.optimizer = None\n","    self.loss_func = loss_func\n","    self.estimate_func = estimate_func\n","    # self.folds_models = []\n","    self.folds_avg_train_loss = []\n","    self.folds_avg_test_loss = []\n","    self.batch_size = batch_size\n","\n","    self.model_class = model_class\n","    self.model_hyperparams = model_hyperparams\n","\n","    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(self.device)\n","    \n","\n","  def train(self, \n","            epochs=1,\n","            lr=5,\n","            k_folds=5,\n","            weight_decay=0\n","    ):\n","    # source: https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/\n","    \n","    kfold = KFold(n_splits=k_folds, shuffle=True)\n","\n","    for fold_num, (train_ids, test_ids) in enumerate(kfold.split(self.dataset)):\n","      print(f\"Fold {fold_num}\")\n","\n","      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","      \n","      trainloader = DataLoader(\n","        self.dataset, \n","        batch_size=self.batch_size,\n","        sampler=train_subsampler\n","      )\n","      \n","      testloader = DataLoader(\n","        self.dataset,\n","        batch_size=self.batch_size,\n","        sampler=test_subsampler\n","      )\n","      \n","      self.model = self.model_class(**self.model_hyperparams)\n","      self.model.to(self.device)\n","      self.optimizer = self.optimizer_class(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n","      \n","      fold_train = 0\n","      fold_test = 0\n","      for epoch in range(epochs):\n","        train_loss = 0\n","        test_loss = 0\n","        \n","        # train\n","        self.model.train()\n","        for features, targets in trainloader:\n","          self.optimizer.zero_grad()\n","\n","          predicts = self.model(features)\n","          loss = self.loss_func(predicts, targets)\n","          loss.backward()\n","          self.optimizer.step()\n","\n","          # train metric\n","          with torch.no_grad():\n","            train_loss += self.estimate_func(predicts, targets)\n","        \n","        # test metric\n","        self.model.eval()\n","        with torch.no_grad():\n","          for features, targets in testloader:\n","            predicts = self.model(features)\n","            test_loss += self.estimate_func(predicts, targets)\n","        \n","        avg_train = train_loss/len(trainloader)\n","        avg_test = test_loss/len(testloader)\n","        fold_train = avg_train\n","        fold_test  = avg_test \n","        print(f\"Fold {fold_num} Epoch {epoch}: train: {avg_train} test: {avg_test}\")\n","\n","      last_fold_train = fold_train #/ epochs\n","      last_fold_test = fold_test #/ epochs\n","\n","      print(f\"Fold {fold_num} ---- Train: {last_fold_train} Test: {last_fold_test} ---- \")\n","\n","      self.folds_avg_train_loss.append(last_fold_train)\n","      self.folds_avg_test_loss.append(last_fold_test)\n","\n","      # del self.model\n","      # del self.optimizer\n","\n","    print(\"Statistics:\")\n","    for i in range(k_folds):\n","      train_l = self.folds_avg_train_loss[i]\n","      test_l = self.folds_avg_test_loss[i]\n","      print(f\"Fold {i}: Train: {train_l} Test: {test_l} diff {abs(train_l - test_l)}\")\n","\n","  def train_pred(self, \n","    epochs=1,\n","    lr=5,\n","    weight_decay=0\n","  ):\n","\n","    trainloader = DataLoader(\n","      dataset=self.dataset, \n","      batch_size=self.batch_size\n","    )\n","\n","    self.model = self.model_class(**self.model_hyperparams)\n","    self.optimizer = self.optimizer_class(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    for epoch in range(epochs):\n","      train_loss = 0\n","      test_loss = 0\n","\n","      # train\n","      for features, targets in trainloader:\n","        self.optimizer.zero_grad()\n","\n","        predicts = self.model(features)\n","        loss = self.loss_func(predicts, targets)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # train metric\n","        with torch.no_grad():\n","          train_loss += self.estimate_func(predicts, targets)\n","\n","      avg_train = train_loss/len(trainloader) \n","      print(f\"Epoch {epoch}: train: {avg_train}\")\n","      \n","\n","target_columns = ['SalePrice']\n","batch_size = 64\n","\n","model_hyperparams = {\n","    'in_features': dataset.shape[1]-1,\n","    'out_features': 1,\n","    'dropout': 0.1\n","}\n","\n","custom_dataset = CustomPandasDataset(\n","    dataset, \n","    target_columns=target_columns,\n",")\n","rmse_loss = RMSLELogLoss()\n","optimizer_class = torch.optim.Adam\n","loss_func_1 = nn.MSELoss()\n","loss_func_2 = nn.L1Loss()\n","\n","trainer = Trainer(\n","  dataset=custom_dataset,\n","  optimizer_class=optimizer_class,\n","  loss_func=loss_func_1, \n","  estimate_func=rmse_loss,\n","  batch_size=batch_size,\n","  model_hyperparams=model_hyperparams,\n","  model_class=MultiLayerModel\n",")\n","# trainer.train(epochs=60, lr=7,)"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yJ7sSkdNUdH","executionInfo":{"status":"ok","timestamp":1638998738669,"user_tz":-180,"elapsed":311755,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"ec394c76-d738-463f-b366-9b92ba4bfc59"},"source":["trainer.train(epochs=35, lr=0.2,)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 0\n","Fold 0 Epoch 0: train: 2.424511671066284 test: 0.21847878396511078\n","Fold 0 Epoch 1: train: 0.22551272809505463 test: 0.18541792035102844\n","Fold 0 Epoch 2: train: 0.15607497096061707 test: 0.17564958333969116\n","Fold 0 Epoch 3: train: 0.12160264700651169 test: 0.19827565550804138\n","Fold 0 Epoch 4: train: 0.12863194942474365 test: 0.19653752446174622\n","Fold 0 Epoch 5: train: 0.12343277782201767 test: 0.17496371269226074\n","Fold 0 Epoch 6: train: 0.12321896106004715 test: 0.19761201739311218\n","Fold 0 Epoch 7: train: 0.11861254274845123 test: 0.19256499409675598\n","Fold 0 Epoch 8: train: 0.11268194019794464 test: 0.18893884122371674\n","Fold 0 Epoch 9: train: 0.1131211444735527 test: 0.20543327927589417\n","Fold 0 Epoch 10: train: 0.11474820226430893 test: 0.20395179092884064\n","Fold 0 Epoch 11: train: 0.10980380326509476 test: 0.2063368856906891\n","Fold 0 Epoch 12: train: 0.11225268989801407 test: 0.1971169412136078\n","Fold 0 Epoch 13: train: 0.11161348968744278 test: 0.1948140263557434\n","Fold 0 Epoch 14: train: 0.10147283226251602 test: 0.2144252359867096\n","Fold 0 Epoch 15: train: 0.11293946206569672 test: 0.2106429785490036\n","Fold 0 Epoch 16: train: 0.11892382800579071 test: 0.18615613877773285\n","Fold 0 Epoch 17: train: 0.10182689130306244 test: 0.1873105764389038\n","Fold 0 Epoch 18: train: 0.11055400222539902 test: 0.2003641575574875\n","Fold 0 Epoch 19: train: 0.1058054119348526 test: 0.1976706087589264\n","Fold 0 Epoch 20: train: 0.10295291244983673 test: 0.2010727971792221\n","Fold 0 Epoch 21: train: 0.11379911750555038 test: 0.21346013247966766\n","Fold 0 Epoch 22: train: 0.11764023452997208 test: 0.209978386759758\n","Fold 0 Epoch 23: train: 0.10732407122850418 test: 0.18563774228096008\n","Fold 0 Epoch 24: train: 0.0970606654882431 test: 0.17613117396831512\n","Fold 0 Epoch 25: train: 0.09795966744422913 test: 0.19298246502876282\n","Fold 0 Epoch 26: train: 0.09570658206939697 test: 0.2083764523267746\n","Fold 0 Epoch 27: train: 0.10813461244106293 test: 0.18078656494617462\n","Fold 0 Epoch 28: train: 0.10310674458742142 test: 0.18179915845394135\n","Fold 0 Epoch 29: train: 0.11676359176635742 test: 0.2030821293592453\n","Fold 0 Epoch 30: train: 0.1038181334733963 test: 0.18710055947303772\n","Fold 0 Epoch 31: train: 0.10234632343053818 test: 0.19837112724781036\n","Fold 0 Epoch 32: train: 0.10576871782541275 test: 0.2013234943151474\n","Fold 0 Epoch 33: train: 0.10597267746925354 test: 0.17552940547466278\n","Fold 0 Epoch 34: train: 0.0927894115447998 test: 0.19154612720012665\n","Fold 0 ---- Train: 0.0927894115447998 Test: 0.19154612720012665 ---- \n","Fold 1\n","Fold 1 Epoch 0: train: 2.5125975608825684 test: 0.22945915162563324\n","Fold 1 Epoch 1: train: 0.394665390253067 test: 0.26673316955566406\n","Fold 1 Epoch 2: train: 0.25770965218544006 test: 0.15682366490364075\n","Fold 1 Epoch 3: train: 0.1759471595287323 test: 0.14511123299598694\n","Fold 1 Epoch 4: train: 0.15705811977386475 test: 0.14645247161388397\n","Fold 1 Epoch 5: train: 0.1524132490158081 test: 0.12288995832204819\n","Fold 1 Epoch 6: train: 0.15651094913482666 test: 0.13087047636508942\n","Fold 1 Epoch 7: train: 0.17255227267742157 test: 0.1448579877614975\n","Fold 1 Epoch 8: train: 0.1408398598432541 test: 0.142823725938797\n","Fold 1 Epoch 9: train: 0.1421099603176117 test: 0.1336916983127594\n","Fold 1 Epoch 10: train: 0.13812397420406342 test: 0.12920373678207397\n","Fold 1 Epoch 11: train: 0.13180753588676453 test: 0.13996104896068573\n","Fold 1 Epoch 12: train: 0.13464802503585815 test: 0.12038808315992355\n","Fold 1 Epoch 13: train: 0.12838725745677948 test: 0.12571918964385986\n","Fold 1 Epoch 14: train: 0.13380689918994904 test: 0.13492660224437714\n","Fold 1 Epoch 15: train: 0.12208514660596848 test: 0.1175542101264\n","Fold 1 Epoch 16: train: 0.11588714271783829 test: 0.1186148151755333\n","Fold 1 Epoch 17: train: 0.12250707298517227 test: 0.11862003803253174\n","Fold 1 Epoch 18: train: 0.11836370825767517 test: 0.12684796750545502\n","Fold 1 Epoch 19: train: 0.1117764413356781 test: 0.11058515310287476\n","Fold 1 Epoch 20: train: 0.11936341226100922 test: 0.11508568376302719\n","Fold 1 Epoch 21: train: 0.11159549653530121 test: 0.11742303520441055\n","Fold 1 Epoch 22: train: 0.10710149258375168 test: 0.12146668881177902\n","Fold 1 Epoch 23: train: 0.1069486141204834 test: 0.1292078197002411\n","Fold 1 Epoch 24: train: 0.11465349048376083 test: 0.12066753208637238\n","Fold 1 Epoch 25: train: 0.13879144191741943 test: 0.13030043244361877\n","Fold 1 Epoch 26: train: 0.11664964258670807 test: 0.11229685693979263\n","Fold 1 Epoch 27: train: 0.11012957990169525 test: 0.11664050072431564\n","Fold 1 Epoch 28: train: 0.10251548886299133 test: 0.11554958671331406\n","Fold 1 Epoch 29: train: 0.10537930577993393 test: 0.10947523266077042\n","Fold 1 Epoch 30: train: 0.10468614846467972 test: 0.12671232223510742\n","Fold 1 Epoch 31: train: 0.1201305240392685 test: 0.1197429895401001\n","Fold 1 Epoch 32: train: 0.09961870312690735 test: 0.11074244976043701\n","Fold 1 Epoch 33: train: 0.10069573670625687 test: 0.11847944557666779\n","Fold 1 Epoch 34: train: 0.10163893550634384 test: 0.11353037506341934\n","Fold 1 ---- Train: 0.10163893550634384 Test: 0.11353037506341934 ---- \n","Fold 2\n","Fold 2 Epoch 0: train: 1.5949270725250244 test: 0.25043877959251404\n","Fold 2 Epoch 1: train: 0.24232463538646698 test: 0.2873894274234772\n","Fold 2 Epoch 2: train: 0.18580250442028046 test: 0.18995404243469238\n","Fold 2 Epoch 3: train: 0.15317746996879578 test: 0.1603711098432541\n","Fold 2 Epoch 4: train: 0.13397911190986633 test: 0.16620220243930817\n","Fold 2 Epoch 5: train: 0.14306652545928955 test: 0.15095117688179016\n","Fold 2 Epoch 6: train: 0.13454365730285645 test: 0.1734200417995453\n","Fold 2 Epoch 7: train: 0.11750447005033493 test: 0.142107293009758\n","Fold 2 Epoch 8: train: 0.11398131400346756 test: 0.15234580636024475\n","Fold 2 Epoch 9: train: 0.12167807668447495 test: 0.14630576968193054\n","Fold 2 Epoch 10: train: 0.1077786237001419 test: 0.1588396281003952\n","Fold 2 Epoch 11: train: 0.10987969487905502 test: 0.14788225293159485\n","Fold 2 Epoch 12: train: 0.10993770509958267 test: 0.13900013267993927\n","Fold 2 Epoch 13: train: 0.11221370100975037 test: 0.1367296427488327\n","Fold 2 Epoch 14: train: 0.10596705973148346 test: 0.14512231945991516\n","Fold 2 Epoch 15: train: 0.11008009314537048 test: 0.13966277241706848\n","Fold 2 Epoch 16: train: 0.10847227275371552 test: 0.15010616183280945\n","Fold 2 Epoch 17: train: 0.11019264906644821 test: 0.1523990035057068\n","Fold 2 Epoch 18: train: 0.11507586389780045 test: 0.14011798799037933\n","Fold 2 Epoch 19: train: 0.11197049915790558 test: 0.14236390590667725\n","Fold 2 Epoch 20: train: 0.10447189956903458 test: 0.13891473412513733\n","Fold 2 Epoch 21: train: 0.10317828506231308 test: 0.1486569494009018\n","Fold 2 Epoch 22: train: 0.10498921573162079 test: 0.1524529606103897\n","Fold 2 Epoch 23: train: 0.10838455706834793 test: 0.15034539997577667\n","Fold 2 Epoch 24: train: 0.1078261062502861 test: 0.14648886024951935\n","Fold 2 Epoch 25: train: 0.10119705647230148 test: 0.15089890360832214\n","Fold 2 Epoch 26: train: 0.0980478897690773 test: 0.1521317958831787\n","Fold 2 Epoch 27: train: 0.10319896042346954 test: 0.13725434243679047\n","Fold 2 Epoch 28: train: 0.10481154173612595 test: 0.16728471219539642\n","Fold 2 Epoch 29: train: 0.10481881350278854 test: 0.15697167813777924\n","Fold 2 Epoch 30: train: 0.10851604491472244 test: 0.16607800126075745\n","Fold 2 Epoch 31: train: 0.14232906699180603 test: 0.16899435222148895\n","Fold 2 Epoch 32: train: 0.12000734359025955 test: 0.12960700690746307\n","Fold 2 Epoch 33: train: 0.11596520245075226 test: 0.19513888657093048\n","Fold 2 Epoch 34: train: 0.12236069142818451 test: 0.15086063742637634\n","Fold 2 ---- Train: 0.12236069142818451 Test: 0.15086063742637634 ---- \n","Fold 3\n","Fold 3 Epoch 0: train: 1.7738149166107178 test: 0.16025470197200775\n","Fold 3 Epoch 1: train: 0.2571934759616852 test: 0.19400285184383392\n","Fold 3 Epoch 2: train: 0.1614242047071457 test: 0.16349530220031738\n","Fold 3 Epoch 3: train: 0.1621108502149582 test: 0.17522858083248138\n","Fold 3 Epoch 4: train: 0.1551690250635147 test: 0.13592274487018585\n","Fold 3 Epoch 5: train: 0.14101430773735046 test: 0.1548314243555069\n","Fold 3 Epoch 6: train: 0.13207551836967468 test: 0.12151781469583511\n","Fold 3 Epoch 7: train: 0.1297421008348465 test: 0.11823447793722153\n","Fold 3 Epoch 8: train: 0.12519562244415283 test: 0.14896802604198456\n","Fold 3 Epoch 9: train: 0.13850566744804382 test: 0.1866559237241745\n","Fold 3 Epoch 10: train: 0.14176982641220093 test: 0.16171957552433014\n","Fold 3 Epoch 11: train: 0.14517100155353546 test: 0.13819767534732819\n","Fold 3 Epoch 12: train: 0.1244409829378128 test: 0.12225481122732162\n","Fold 3 Epoch 13: train: 0.11898462474346161 test: 0.1284193992614746\n","Fold 3 Epoch 14: train: 0.11752220243215561 test: 0.13247458636760712\n","Fold 3 Epoch 15: train: 0.11819317191839218 test: 0.10314466804265976\n","Fold 3 Epoch 16: train: 0.12313811480998993 test: 0.1371220499277115\n","Fold 3 Epoch 17: train: 0.1265355944633484 test: 0.13995404541492462\n","Fold 3 Epoch 18: train: 0.11820077896118164 test: 0.1329749971628189\n","Fold 3 Epoch 19: train: 0.1206694170832634 test: 0.11866175383329391\n","Fold 3 Epoch 20: train: 0.10538838803768158 test: 0.13663248717784882\n","Fold 3 Epoch 21: train: 0.11945267766714096 test: 0.13100147247314453\n","Fold 3 Epoch 22: train: 0.11246657371520996 test: 0.16161714494228363\n","Fold 3 Epoch 23: train: 0.11395333707332611 test: 0.12189143896102905\n","Fold 3 Epoch 24: train: 0.10091979056596756 test: 0.1392413228750229\n","Fold 3 Epoch 25: train: 0.12453922629356384 test: 0.18433299660682678\n","Fold 3 Epoch 26: train: 0.1238095760345459 test: 0.1307322233915329\n","Fold 3 Epoch 27: train: 0.11619871854782104 test: 0.11745579540729523\n","Fold 3 Epoch 28: train: 0.11234529316425323 test: 0.10459671169519424\n","Fold 3 Epoch 29: train: 0.1081031784415245 test: 0.11207864433526993\n","Fold 3 Epoch 30: train: 0.10471809655427933 test: 0.11474747955799103\n","Fold 3 Epoch 31: train: 0.12200649082660675 test: 0.11640962213277817\n","Fold 3 Epoch 32: train: 0.11567434668540955 test: 0.18801115453243256\n","Fold 3 Epoch 33: train: 0.1447998285293579 test: 0.2097884714603424\n","Fold 3 Epoch 34: train: 0.13141733407974243 test: 0.1527159959077835\n","Fold 3 ---- Train: 0.13141733407974243 Test: 0.1527159959077835 ---- \n","Fold 4\n","Fold 4 Epoch 0: train: 1.8854730129241943 test: 0.21166947484016418\n","Fold 4 Epoch 1: train: 0.2359839826822281 test: 0.13369563221931458\n","Fold 4 Epoch 2: train: 0.15628443658351898 test: 0.1359025090932846\n","Fold 4 Epoch 3: train: 0.18412035703659058 test: 0.14901430904865265\n","Fold 4 Epoch 4: train: 0.1592870056629181 test: 0.1299993097782135\n","Fold 4 Epoch 5: train: 0.1389489620923996 test: 0.11827941238880157\n","Fold 4 Epoch 6: train: 0.13484704494476318 test: 0.13016733527183533\n","Fold 4 Epoch 7: train: 0.14562298357486725 test: 0.1325896829366684\n","Fold 4 Epoch 8: train: 0.1233435794711113 test: 0.12095358222723007\n","Fold 4 Epoch 9: train: 0.12913937866687775 test: 0.15529611706733704\n","Fold 4 Epoch 10: train: 0.12684404850006104 test: 0.12216155976057053\n","Fold 4 Epoch 11: train: 0.12058776617050171 test: 0.11850976198911667\n","Fold 4 Epoch 12: train: 0.11455097794532776 test: 0.13810761272907257\n","Fold 4 Epoch 13: train: 0.12025948613882065 test: 0.12894023954868317\n","Fold 4 Epoch 14: train: 0.1223873496055603 test: 0.13308225572109222\n","Fold 4 Epoch 15: train: 0.13163332641124725 test: 0.11504735797643661\n","Fold 4 Epoch 16: train: 0.12119151651859283 test: 0.12757965922355652\n","Fold 4 Epoch 17: train: 0.11141276359558105 test: 0.118970587849617\n","Fold 4 Epoch 18: train: 0.1048445776104927 test: 0.11977171152830124\n","Fold 4 Epoch 19: train: 0.11342969536781311 test: 0.1082439199090004\n","Fold 4 Epoch 20: train: 0.10887350887060165 test: 0.11597415059804916\n","Fold 4 Epoch 21: train: 0.11235825717449188 test: 0.15183457732200623\n","Fold 4 Epoch 22: train: 0.13046795129776 test: 0.11324256658554077\n","Fold 4 Epoch 23: train: 0.1325259506702423 test: 0.1418900489807129\n","Fold 4 Epoch 24: train: 0.12618157267570496 test: 0.1126495748758316\n","Fold 4 Epoch 25: train: 0.11171086132526398 test: 0.11790845543146133\n","Fold 4 Epoch 26: train: 0.11096291989088058 test: 0.1336812973022461\n","Fold 4 Epoch 27: train: 0.11419980227947235 test: 0.13053464889526367\n","Fold 4 Epoch 28: train: 0.10677020251750946 test: 0.11438661068677902\n","Fold 4 Epoch 29: train: 0.1078939288854599 test: 0.11905694007873535\n","Fold 4 Epoch 30: train: 0.1045156791806221 test: 0.12436588853597641\n","Fold 4 Epoch 31: train: 0.103300079703331 test: 0.10876643657684326\n","Fold 4 Epoch 32: train: 0.0976146012544632 test: 0.1652771532535553\n","Fold 4 Epoch 33: train: 0.11223924905061722 test: 0.13739216327667236\n","Fold 4 Epoch 34: train: 0.09891079366207123 test: 0.12369681894779205\n","Fold 4 ---- Train: 0.09891079366207123 Test: 0.12369681894779205 ---- \n","Statistics:\n","Fold 0: Train: 0.0927894115447998 Test: 0.19154612720012665 diff 0.09875671565532684\n","Fold 1: Train: 0.10163893550634384 Test: 0.11353037506341934 diff 0.0118914395570755\n","Fold 2: Train: 0.12236069142818451 Test: 0.15086063742637634 diff 0.028499945998191833\n","Fold 3: Train: 0.13141733407974243 Test: 0.1527159959077835 diff 0.021298661828041077\n","Fold 4: Train: 0.09891079366207123 Test: 0.12369681894779205 diff 0.024786025285720825\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"id":"_pO_tvGoIivC","executionInfo":{"status":"error","timestamp":1638974101089,"user_tz":-180,"elapsed":41896,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"fa5d7ecc-ced1-400d-e183-e23cd1ec94c7"},"source":["trainer.train_pred(epochs=20, lr=0.1, weight_decay=0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train: 1.9990402460098267\n","Epoch 1: train: 0.21742010116577148\n","Epoch 2: train: 0.14562001824378967\n","Epoch 3: train: 0.1316131204366684\n","Epoch 4: train: 0.13031113147735596\n","Epoch 5: train: 0.1274307370185852\n","Epoch 6: train: 0.12191027402877808\n","Epoch 7: train: 0.11977142095565796\n","Epoch 8: train: 0.12125454097986221\n","Epoch 9: train: 0.12230141460895538\n","Epoch 10: train: 0.11907583475112915\n","Epoch 11: train: 0.11844606697559357\n","Epoch 12: train: 0.11834661662578583\n","Epoch 13: train: 0.11846049129962921\n","Epoch 14: train: 0.11371959000825882\n","Epoch 15: train: 0.11063180863857269\n","Epoch 16: train: 0.11019828170537949\n","Epoch 17: train: 0.10850803554058075\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-c892112a9eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-78-9576efa8d8a4>\u001b[0m in \u001b[0;36mtrain_pred\u001b[0;34m(self, epochs, lr, weight_decay)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ef63dcfb7e74>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdata_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdata_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2835\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2836\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2837\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2838\u001b[0m             )\n\u001b[1;32m   2839\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# we will try to copy be-definition here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;31m# perf shortcut as this is the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmaybe_castable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_castable\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_timedelta64_ns_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_POSSIBLY_CAST_DTYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# append bit counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_name_includes_bit_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_includes_bit_suffix\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# implied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflexible\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_isunsized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# unspecified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \"\"\"\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0marg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ku2GhgFS5JD7"},"source":["# model=MultiLayerModel(\n","#     features=features_size, \n","#     targets=targets_size\n","# )\n","# model = nn.Linear(in_features=features_size, out_features=targets_size)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","# loss_func = nn.MSELoss()\n","# acc_func = RMSLELogLoss()\n","\n","# for ep in range(15):\n","#   total_loss = 0\n","#   total_elements = 0\n","#   for fea, tar in train_dataloader:\n","#     pred = model(fea)\n","#     loss = loss_func(pred, tar)\n","#     loss.backward()\n","#     optimizer.step()\n","#     optimizer.zero_grad()\n","#     with torch.no_grad():\n","#       total_loss += acc_func(pred, tar)\n","#       total_elements += len(tar)\n","#   print(total_loss/total_elements)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVRe5OoIGpJg"},"source":["## submit"]},{"cell_type":"code","metadata":{"id":"Xe7JhkkpGrxU","executionInfo":{"status":"ok","timestamp":1638997518595,"user_tz":-180,"elapsed":814,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["dataset_submit = pd.read_csv(\"./data/HousePricesKaggle/test.csv\")\n","\n","\n","drop_columns = 'Id'.split()\n","dataset_submit = dataset_submit.drop(columns=drop_columns)\n","\n","select_columns = dataset_submit.dtypes[dataset_submit.dtypes != 'object'].index.tolist()\n","# select_columns = list(set(select_columns) - set(['SalePrice']))\n","\n","dataset_submit[select_columns] = dataset_submit[select_columns].apply(lambda x: (x - x.mean()) / (x.std()))\n","target_columns = ['SalePrice']\n","dataset_submit[select_columns] = dataset_submit[select_columns].fillna(dataset_submit[select_columns].mean() / dataset_submit[select_columns].std())\n","\n","OH_cols_submit = pd.DataFrame(OH_encoder.transform(dataset_submit[object_columns]))\n","\n","OH_cols_submit.index = dataset_submit.index\n","num_X_submit = dataset_submit.drop(object_columns, axis=1)\n","\n","dataset_submit = pd.concat([num_X_submit, OH_cols_submit], axis=1)\n","\n","batch_size = len(dataset_submit)\n","submit_dataset = CustomPandasDataset(\n","    dataset=dataset_submit,\n","    target_columns=target_columns,\n","    infer=True\n",")\n","\n","submit_dataloader = DataLoader(\n","    dataset=submit_dataset, \n","    batch_size=batch_size, \n","    shuffle=False,\n",")\n","\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx2bDBmfOPYG"},"source":["dataset_submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27phUdZ4H6eG","executionInfo":{"status":"ok","timestamp":1638997554376,"user_tz":-180,"elapsed":960,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}}},"source":["with torch.no_grad():\n","  features, targets = next(iter(submit_dataloader))\n","  # print(features)\n","  trainer.model.eval()\n","  preds = trainer.model(features).cpu().numpy().reshape(-1)\n","  # print(preds)\n","\n","  test_data_subm = pd.read_csv(\"./data/HousePricesKaggle/test.csv\")\n","  test_data_subm = test_data_subm[['Id']]\n","  test_data_subm['SalePrice'] = preds\n","\n","  test_data_subm.to_csv('./data/HousePricesKaggle/submission_my.csv', index=False)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yq5qA4JEMIQv","executionInfo":{"status":"ok","timestamp":1638281577437,"user_tz":-180,"elapsed":284,"user":{"displayName":"dim web","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03939316973290678021"}},"outputId":"3710bf40-13ea-4d1f-fb28-25606c1c8edb"},"source":["np.array([[1], [2], [3]]).reshape(-1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":133}]}]}